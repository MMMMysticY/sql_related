# SQL技巧
- [SQL技巧](#sql技巧)
  - [SQL语法部分](#sql语法部分)
    - [group by去重与distinct去重](#group-by去重与distinct去重)
    - [自增列](#自增列)
  - [HDFS交互部分](#hdfs交互部分)
    - [与HDFS(AFS)的IO操作](#与hdfsafs的io操作)
      - [Input](#input)
      - [Output](#output)
    - [HDFS表写入MySQL](#hdfs表写入mysql)

## SQL语法部分
### group by去重与distinct去重
group by和distinct都可以进行去重。
1. distinct仅可用于去重，具体内容见[基础.md](基础.md)的distinct小节。
2. group by按照部分列进行分组之后，保证了对该类列的去重，因为group by每分的一组在最终的sql中都只能存在一行数据。

```sql
select distinct gender, city from patients; -- 对性别和城市同时去重
select gender, city from patients group by gender, city; -- group by去重
```
### 自增列
自增列作为主键。关键词为AUTO_INCREMENT，往往附带了主键PRIMARY KEY。  
语法：
```sql
create table xxx(
    id INT AUTO_INCREMENT PRIMARY KEY,
    yyy
);
```
优点：
1. 唯一性。插入新行时自增列自动增加，从而保证唯一，满足主键的要求；
2. 简化数据维护。自动分配了唯一的标识符，可以减少出错的可能性；
3. 提高性能。往往可以成为索引列；
4. 方柏霓操作。能够解决重复插入的报错问题、能够使用where的> <操作来进行选取部分数据。

## HDFS交互部分
以下内容应为HIVE表的功能
### 与HDFS(AFS)的IO操作
IO操作需要先设定读写的地址，不同HDFS系统不一，类似语法应该为定义性。  
样例：  
```sql
set 'user_writer_ugi:afs://xxx'=xxx,xxx;
set 'user_read_ugi:afs://'=xxx,xxx;
```
#### Input
使用SQL命令需要读入并操作HDFS(AFS)系统中的数据时，需要对文件进行读取，创建（临时）的表（视图）作为对象进行后续操作。  
```sql
create (TEMPORARY) table (view) my_table    -- 选择读取文件的数据形式：临时/非临时的表/视图
using csv (parquet text)                    -- 选择文件读取的数据形式 （数据如何存储应该如何读取）
location 'afs://xxx'                        -- 选择文件数据存储的位置
options(
    'delimiter' = '\t',                     -- 选择文件的存储分隔符 一般为\t或者,等
    'header' = 'true',                      -- 选择文件的存储方式是否存在表头(列名)
    'recursiveFileLookup' = 'true',         -- 是否递归性地读取文件夹 若location中有多个文件夹的数据，如多日的数据，选择参数为true即可对其整体进行读取 ☆很有用
    'hadoop.job.ugi' = 'xxx'                -- 选择任务队列
);                                          -- 独立完成的语句 需要加分号
```
在该命令之后，后续的SQL代码可以对my_table对象进行操作。
#### Output
将SQL命令执行的结果写入HDFS(AFS)系统中，需要提前对各种信息进行配置。  
```sql
insert overwrite directory 'afs://xxx'      -- 选择要写入的位置
using csv (parquet text)                    -- 选择文件存储的数据形式
options(
    'delimiter' = '\t',                     -- 选择文件的存储分隔符 一般为\t或者,等
    'header' = 'true',                      -- 选择文件的存储方式是否存在表头(列名)
    'hadoop.job.ugi' = 'xxx'                -- 选择任务队列
)                                           -- 语句没有完成 不能加分号

select xxx;                                 -- 配上后续的sql命令才算一条完整语句
```

### HDFS表写入MySQL
hdfs或csv文件要写入MySQL表中时，可以使用mysql-connector库函数进行处理。  
```python
import mysql.connector
db = mysql.connector.connect(user='', password='', host='', port='', database='')
cursor = db.cursor()
query = f"insert into xxx"
cursor.executemany(query, values)
db.commit()
db.close()
```
