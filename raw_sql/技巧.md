# SQL技巧
- [SQL技巧](#sql技巧)
	- [SQL语法部分](#sql语法部分)
		- [group by去重与distinct去重](#group-by去重与distinct去重)
		- [count(1)](#count1)
	- [SQL功能](#sql功能)
		- [数值数据分桶计算频次](#数值数据分桶计算频次)
	- [HDFS交互部分](#hdfs交互部分)
		- [与HDFS(AFS)的IO操作](#与hdfsafs的io操作)
			- [Input](#input)
			- [Output](#output)

## SQL语法部分
### group by去重与distinct去重
group by和distinct都可以进行去重。
1. distinct仅可用于去重，具体内容见[基础.md](基础.md)的distinct小节。
2. group by按照部分列进行分组之后，保证了对该类列的去重，因为group by每分的一组在最终的sql中都只能存在一行数据。

```sql
select distinct gender, city from patients; -- 对性别和城市同时去重
select gender, city from patients group by gender, city; -- group by去重
```
### count(1)
count(1)方法用来在有where的情况下统计结果的行数的语法，比count(*)往往更高效
```sql
select 
  count(1) as row_num,
  avg(score) as avg_score
from students
where score > 10
```
## SQL功能
### 数值数据分桶计算频次
对于规模不大的数据而言，数据作为pd.DataFrame对象进行构建，而后结合cut分箱、value_counts统计频次和hist作图进行处理。**然而，在大规模数据中，以内存形式构建DataFrame对象并进行处理几乎不太可行，往往需要借助SQL方法进行大数据高效处理。**  
处理方法：
1. 将数值型数据使用round方法进行四舍五入；例如：以round(2)进行处理时，则相当于以0.01为一个桶进行分箱；
2. 以上述字段进行group by后，结合count(*)方法进行频次统计；
3. 最后导出结果进行pandas或matplotlib可视化。

样例：
```sql
select
    price_buckets as buckets,
    count(*) as freq
from(
	select
		round(price, 2) as price_buckets
		-- 按照0.01为一个桶进行
	from(
		select
			price
		from
			order_table
		where
			price > 20
	)
)
group by price_buckets
order by buckets
```
后续作图分析不在本节中考虑
## HDFS交互部分
以下内容应为HIVE表的功能
### 与HDFS(AFS)的IO操作
IO操作需要先设定读写的地址，不同HDFS系统不一，类似语法应该为定义性。  
样例：  
```sql
set 'user_writer_ugi:afs://xxx'=xxx,xxx;
set 'user_read_ugi:afs://'=xxx,xxx;
```
#### Input
使用SQL命令需要读入并操作HDFS(AFS)系统中的数据时，需要对文件进行读取，创建（临时）的表（视图）作为对象进行后续操作。  
```sql
create (TEMPORARY) table (view) my_table    -- 选择读取文件的数据形式：临时/非临时的表/视图
using csv (parquet text)                    -- 选择文件读取的数据形式 （数据如何存储应该如何读取）
location 'afs://xxx'                        -- 选择文件数据存储的位置
options(
    'delimiter' = '\t',                     -- 选择文件的存储分隔符 一般为\t或者,等
    'header' = 'true',                      -- 选择文件的存储方式是否存在表头(列名)
    'recursiveFileLookup' = 'true',         -- 是否递归性地读取文件夹 若location中有多个文件夹的数据，如多日的数据，选择参数为true即可对其整体进行读取 ☆很有用
    'hadoop.job.ugi' = 'xxx'                -- 选择任务队列
);                                          -- 独立完成的语句 需要加分号
```
在该命令之后，后续的SQL代码可以对my_table对象进行操作。
#### Output
将SQL命令执行的结果写入HDFS(AFS)系统中，需要提前对各种信息进行配置。  
```sql
insert overwrite directory 'afs://xxx'      -- 选择要写入的位置
using csv (parquet text)                    -- 选择文件存储的数据形式
options(
    'delimiter' = '\t',                     -- 选择文件的存储分隔符 一般为\t或者,等
    'header' = 'true',                      -- 选择文件的存储方式是否存在表头(列名)
    'hadoop.job.ugi' = 'xxx'                -- 选择任务队列
)                                           -- 语句没有完成 不能加分号

select xxx;                                 -- 配上后续的sql命令才算一条完整语句
```